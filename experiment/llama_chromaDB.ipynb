{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GEMINI_API_KEY']=os.getenv(\"GEMINI_API_KEY\")\n",
    "# os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=Gemini(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "embed_model=GeminiEmbedding(model_name=\"models/embedding-001\",api_key=os.environ[\"GEMINI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.ingestion import IngestionPipeline\n",
    "# from llama_index.core.node_parser import SentenceSplitter\n",
    "# from llama_index.core.retrievers import VectorContextRetriever\n",
    "# from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# documents=SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_content = \"\"\n",
    "    tables = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Extract text from all pages\n",
    "        for page in pdf.pages:\n",
    "            text_content += page.extract_text() + \"\\n\"  # Extract text\n",
    "        \n",
    "        # Extract tables from each page\n",
    "        for page in pdf.pages:\n",
    "            page_tables = page.extract_tables()\n",
    "            for table in page_tables:\n",
    "                tables.append(table)\n",
    "    \n",
    "    return text_content, tables\n",
    "\n",
    "# Convert the extracted tables into a string format suitable for LlamaIndex\n",
    "def format_tables_as_text(tables):\n",
    "    table_text = \"\"\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            # Handle None values in the table row by replacing with empty string\n",
    "            row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            table_text += \"\\t\".join(row) + \"\\n\"  # Tab-separated row\n",
    "        table_text += \"\\n---\\n\"  # Separator between tables (optional)\n",
    "    return table_text\n",
    "\n",
    "# Extract content and tables from a PDF\n",
    "pdf_path = \"data/paper.pdf\"\n",
    "text_content, tables = extract_pdf_content(pdf_path)\n",
    "\n",
    "# Format tables into text\n",
    "table_text = format_tables_as_text(tables)\n",
    "\n",
    "# Combine text and table data into a single document\n",
    "combined_text = text_content + \"\\n\\n\" + \"Tables extracted from the PDF:\\n\" + table_text\n",
    "\n",
    "# Now, we can treat `combined_text` as a document to load into LlamaIndex\n",
    "documents = [combined_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses future improvements to an IoT-based health monitoring system.  These include adding more health metrics, proactive wellness management features with statistical notifications, a multi-modal approach for various age groups, personalized fitness recommendations, and long-term progress tracking.  The goal is to create a system that significantly improves remote health management.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load from disk\n",
    "load_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Fetch the collection\n",
    "chroma_collection = load_client.get_collection(\"quickstart\")\n",
    "\n",
    "# Fetch the vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Get the index from the vector store\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store\n",
    ")\n",
    "\n",
    "test_query_engine = index.as_query_engine()\n",
    "response = test_query_engine.query(\"give me brief about the text\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Result: The data shows a comparison of three machine learning models: Linear Regression, Decision Tree Regressor, and XGBoost Regressor.  Each model's performance is evaluated using the R-squared score (R2 Score) and Mean Squared Error (MSE).  The XGBoost Regressor achieved the highest R2 score (0.9674) and the lowest MSE (0.0033), indicating the best performance among the three.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from llama_index.core import Document,VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "# Step 1: Extract text and tables from the PDF using pdfplumber\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_content = \"\"\n",
    "    tables = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Extract text from all pages\n",
    "        for page in pdf.pages:\n",
    "            text_content += page.extract_text() + \"\\n\"  # Extract text\n",
    "        \n",
    "        # Extract tables from each page\n",
    "        for page in pdf.pages:\n",
    "            page_tables = page.extract_tables()\n",
    "            for table in page_tables:\n",
    "                tables.append(table)\n",
    "    \n",
    "    return text_content, tables\n",
    "\n",
    "# Step 2: Convert the extracted tables into a string format suitable for LlamaIndex\n",
    "def format_tables_as_text(tables):\n",
    "    table_text = \"\"\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            # Handle None values in the table row by replacing with empty string\n",
    "            row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            table_text += \"\\t\".join(row) + \"\\n\"  # Tab-separated row\n",
    "        table_text += \"\\n---\\n\"  # Separator between tables (optional)\n",
    "    return table_text\n",
    "\n",
    "# Step 3: Create a ChromaDB client and collection\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = client.get_or_create_collection(\"pdf_documents\")\n",
    "\n",
    "# Step 4: Create the vector store using ChromaDB\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Step 5: Extract text and tables from the PDF\n",
    "pdf_path = \"data/paper.pdf\"\n",
    "text_content, tables = extract_pdf_content(pdf_path)\n",
    "\n",
    "# Step 6: Format tables into text and combine them with the main text content\n",
    "table_text = format_tables_as_text(tables)\n",
    "combined_text = text_content + \"\\n\\n\" + \"Tables extracted from the PDF:\\n\" + table_text\n",
    "\n",
    "# Step 7: Wrap the combined text into LlamaIndex BaseDocument format\n",
    "documents = [Document(text=combined_text, doc_id=\"pdf_document_1\")]\n",
    "\n",
    "# Step 8: Create the storage context with the ChromaVectorStore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Step 9: Create an index from the documents and store it in ChromaDB\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# Optionally save the index to disk\n",
    "# index.save_to_disk(\"index_path\")\n",
    "\n",
    "# Step 10: Query the index for specific data\n",
    "query = \"Find tables and content in the given data\"\n",
    "response = index.as_query_engine().query(query)\n",
    "print(\"Query Result:\", response)\n",
    "\n",
    "# # Optionally: You can perform a query on ChromaDB directly if you need more flexibility:\n",
    "# results = chroma_collection.query(query_texts=[query], n_results=1)\n",
    "# print(\"ChromaDB Query Result:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Result: S. No\tMachine Learning Model\tR2 Score\tMSE\n",
      "1\tLinear regression\t0.9481\t0.0053\n",
      "2\tDecision tree Regressor\t0.9574\t0.0426\n",
      "3\tXGBoost Regressor\t0.9674\t0.0033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract text and tables from the PDF using pdfplumber\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_content = \"\"\n",
    "    tables = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Extract text from all pages\n",
    "        for page in pdf.pages:\n",
    "            text_content += page.extract_text() + \"\\n\"  # Extract text\n",
    "        \n",
    "        # Extract tables from each page\n",
    "        for page in pdf.pages:\n",
    "            page_tables = page.extract_tables()\n",
    "            for table in page_tables:\n",
    "                tables.append(table)\n",
    "    \n",
    "    return text_content, tables\n",
    "\n",
    "# Step 2: Convert the extracted tables into a string format suitable for LlamaIndex\n",
    "def format_tables_as_text(tables):\n",
    "    table_text = \"\"\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            # Handle None values in the table row by replacing with empty string\n",
    "            row = [str(cell) if cell is not None else \"\" for cell in row]\n",
    "            table_text += \"\\t\".join(row) + \"\\n\"  # Tab-separated row\n",
    "        table_text += \"\\n---\\n\"  # Separator between tables (optional)\n",
    "    return table_text\n",
    "\n",
    "# Step 3: Create a ChromaDB client and collection\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = client.get_or_create_collection(\"pdf_documents\")\n",
    "\n",
    "# Step 4: Create the vector store using ChromaDB\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Step 5: Extract text and tables from the PDF\n",
    "pdf_path = \"data/paper.pdf\"\n",
    "text_content, tables = extract_pdf_content(pdf_path)\n",
    "\n",
    "# Step 6: Format tables into text and combine them with the main text content\n",
    "table_text = format_tables_as_text(tables)\n",
    "\n",
    "# Step 7: Split documents into separate text and table entries for better indexing\n",
    "documents = []\n",
    "\n",
    "# Add text content as a separate document\n",
    "documents.append(Document(text=text_content, doc_id=\"pdf_text\"))\n",
    "\n",
    "# Add table content as separate documents (one for each table)\n",
    "for i, table in enumerate(tables):\n",
    "    table_doc_text = format_tables_as_text([table])\n",
    "    documents.append(Document(text=table_doc_text, doc_id=f\"pdf_table_{i+1}\"))\n",
    "\n",
    "# Step 8: Create the storage context with the ChromaVectorStore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Step 9: Create an index from the documents and store it in ChromaDB\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# Optionally save the index to disk (if you want to persist the index)\n",
    "# storage_context.persist(persist_dir=\"./storage\")\n",
    "\n",
    "# Step 10: Query the index for specific data\n",
    "query = \"Find tables and give it exactly as it is\"\n",
    "response = index.as_query_engine().query(query)\n",
    "print(\"Query Result:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Result: One.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"how many tables\"\n",
    "response = index.as_query_engine().query(query)\n",
    "print(\"Query Result:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
