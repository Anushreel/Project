{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GEMINI_API_KEY']=os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ['GROQ_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf loading\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/paper.pdf\")\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "with pdfplumber.open(\"data/paper.pdf\") as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    table = first_page.extract_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text snippets, I can partially recreate the structure of the tables, but the content is missing.  The text only describes *what* the tables are about, not the data *within* them.\n",
      "\n",
      "**Table I:**\n",
      "\n",
      "This table is referenced as containing information gathered from the developed hardware used on 10 people.  The data includes:\n",
      "\n",
      "* Heart Rate\n",
      "* Blood Oxygen Levels\n",
      "* Limb Movement\n",
      "* Body Temperature\n",
      "\n",
      "Therefore, Table I would likely look something like this (with example data, as the real data isn't provided):\n",
      "\n",
      "| Person | Heart Rate (bpm) | Blood Oxygen (%) | Limb Movement (description) | Body Temperature (°C) |\n",
      "|---|---|---|---|---|\n",
      "| 1 | 72 | 98 | Resting | 36.7 |\n",
      "| 2 | 80 | 99 | Shifting | 36.5 |\n",
      "| ... | ... | ... | ... | ... |\n",
      "| 10 | 65 | 97 | Resting | 37.0 |\n",
      "\n",
      "\n",
      "**Table II:**\n",
      "\n",
      "This table is described as a comparison of existing studies with health vitals versus the proposed solution.  The only column header provided is \"Reference No\".  It's impossible to reconstruct the other columns without more information.  The structure would likely be something like this:\n",
      "\n",
      "| Reference No | Existing Study (Description/Metrics) | Proposed Solution (Description/Metrics) |  ... (Other comparison columns) |\n",
      "|---|---|---|---|\n",
      "| 1 |  ... | ... | ... |\n",
      "| 2 |  ... | ... | ... |\n",
      "| ... | ... | ... | ... |\n",
      "\n",
      "\n",
      "The text mentions training ML models on the \"SaYoPillow database\" and testing/validating on a recorded database.  This information likely belongs *within* Table II, comparing the performance or features of different approaches.  However, without the actual table data, I can only speculate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#Load the models\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "#Load the PDF and create chunks\n",
    "loader = PyPDFLoader(\"data/paper.pdf\")\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "\n",
    "#Turn the chunks into embeddings and store them in Chroma\n",
    "vectordb=Chroma.from_documents(pages,embeddings)\n",
    "\n",
    "#Configure Chroma as a retriever with top_k=5\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "#Create the retrieval chain\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Answer based on the context provided. \n",
    "context: {context}\n",
    "input: {input}\n",
    "answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "#Invoke the retrieval chain\n",
    "response=retrieval_chain.invoke({\"input\":\"can you recreate the tables present and show me as it is\"})\n",
    "\n",
    "#Print the answer to the question\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLE II.  COMPARISION TABLE OF EXISTING STUDIES WITH HEALTH VITALS VS PROPOSED SOLUTION . \n",
      "\n",
      "| Reference No. | Dataset | Heart rate | Sp O2 | Body Temp | Sleep hours | Limb Accuracy |\n",
      "|---|---|---|---|---|---|---|\n",
      "| Ref 5 | Private | α | α | α | Nan | Nan |\n",
      "| Ref 6 | Private | α | α |  | Nan | Nan |\n",
      "| Ref 7 | Private | α | α |  | Nan | Nan |\n",
      "| Ref 8 | Private | α |  |  | Nan | Nan |\n",
      "| Ref 9 | Private | α | α |  | Nan | Nan |\n",
      "| Ref 10 | Private | α | α | α | α | Nan |\n",
      "| Proposed solution <br> SaYo Pillow | SaYo Pillow | α | α | α | α | α | 96.7 % |\n",
      "\n",
      "\n",
      "TABLE IV. (Repeated - Identical to TABLE II)  COMPARISION TABLE OF EXISTING STUDIES WITH HEALTH VITALS VS PROPOSED SOLUTION . \n",
      "\n",
      "| Reference No. | Dataset | Heart rate | Sp O2 | Body Temp | Sleep hours | Limb Accuracy |\n",
      "|---|---|---|---|---|---|---|\n",
      "| Ref 5 | Private | α | α | α | Nan | Nan |\n",
      "| Ref 6 | Private | α | α |  | Nan | Nan |\n",
      "| Ref 7 | Private | α | α |  | Nan | Nan |\n",
      "| Ref 8 | Private | α |  |  | Nan | Nan |\n",
      "| Ref 9 | Private | α | α |  | Nan | Nan |\n",
      "| Ref 10 | Private | α | α | α | α | Nan |\n",
      "| Proposed solution <br> SaYo Pillow | SaYo Pillow | α | α | α | α | α | 96.7 % | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Load the models\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# Load the PDF using pdfplumber for table and text extraction\n",
    "with pdfplumber.open(\"data/paper.pdf\") as pdf:\n",
    "    all_text = \"\"\n",
    "    pages = []\n",
    "\n",
    "    # Loop through each page in the PDF\n",
    "    for page in pdf.pages:\n",
    "        # Extract text\n",
    "        text = page.extract_text()\n",
    "        all_text += text + \"\\n\"\n",
    "\n",
    "        # Extract tables from each page\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            # Filter out rows containing None\n",
    "            filtered_table = [row for row in table if None not in row]\n",
    "            table_text = \"\\n\".join([\"\\t\".join(row) for row in filtered_table])\n",
    "            all_text += \"\\n\" + table_text + \"\\n\"\n",
    "\n",
    "        # Add page content to the list\n",
    "        pages.append(text + \"\\n\" + table_text)\n",
    "\n",
    "# Now, create chunks and embeddings\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "# Convert text chunks to Document objects\n",
    "documents = [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# Turn the chunks into embeddings and store them in Chroma\n",
    "vectordb = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Configure Chroma as a retriever with top_k=5\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Create the retrieval chain\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Answer based on the context provided. \n",
    "context: {context}\n",
    "input: {input}\n",
    "answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# Invoke the retrieval chain\n",
    "response = retrieval_chain.invoke({\"input\": \"Can you recreate the tables present and show me as it is\"})\n",
    "\n",
    "# Print the answer to the question\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are the tables as they were provided:\n",
      "\n",
      "Table I. Data Acquisition\n",
      "\n",
      "| Data Type | Variables |\n",
      "| --- | --- |\n",
      "| Heart rate | BPM |\n",
      "| Blood oxygen levels | SpO2 |\n",
      "| Limb movement | Accuracy (%) |\n",
      "| Body temperature | Temperature (°C) |\n",
      "\n",
      "Table II. Comparison Table of Existing Studies with Health Vitals vs Proposed Solution\n",
      "\n",
      "| Reference No. | Dataset | Heart rate | SpO2 | Body Temp | Sleep hours | Limb Accuracy |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Ref 5 | Private | ✓ | ✓ | ✓ | Nan |  |\n",
      "| Ref 6 | Private | ✓ | ✓ | ✓ | Nan |  |\n",
      "| Ref 7 | Private | ✓ | ✓ | ✓ | Nan |  |\n",
      "| Ref 8 | Private | ✓ |  |  | Nan |  |\n",
      "| Ref 9 | Private | ✓ | ✓ | ✓ | Nan |  |\n",
      "| Ref 10 | Private | ✓ | ✓ | ✓ | ✓ | ✓ |\n",
      "| Proposed solution | SaYo Pillow | ✓ | ✓ | ✓ | ✓ | 96.7% |\n",
      "\n",
      "Note: In the tables, \"Nan\" represents missing data, and \"✓\" indicates that the data is present. The proposed solution, SaYo Pillow, has been tested and validated using the recorded database and a similar existing database called the SaYoPillow database. The data used in these tables was acquired from 10 people using the developed hardware, which gathers information on heart rate, blood oxygen levels, limb movement, and body temperature.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from groq_sdk import GroqModel, GroqEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import pdfplumber\n",
    "\n",
    "# Initialize Groq LLM and Embeddings\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"]\n",
    ")\n",
    "# embeddings = GroqEmbeddings(\n",
    "#     model=\"groq-embedding-1\",  # Replace with the correct embedding model\n",
    "#     api_key=os.environ[\"GROQ_API_KEY\"]\n",
    "# )\n",
    "\n",
    "# Load the PDF using pdfplumber for table and text extraction\n",
    "with pdfplumber.open(\"data/paper.pdf\") as pdf:\n",
    "    all_text = \"\"\n",
    "    pages = []\n",
    "\n",
    "    # Loop through each page in the PDF\n",
    "    for page in pdf.pages:\n",
    "        # Extract text\n",
    "        text = page.extract_text()\n",
    "        all_text += text + \"\\n\"\n",
    "\n",
    "        # Extract tables from each page\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            # Filter out rows containing None\n",
    "            filtered_table = [row for row in table if None not in row]\n",
    "            table_text = \"\\n\".join([\"\\t\".join(row) for row in filtered_table])\n",
    "            all_text += \"\\n\" + table_text + \"\\n\"\n",
    "\n",
    "        # Add page content to the list\n",
    "        pages.append(text + \"\\n\" + table_text)\n",
    "\n",
    "# Create text chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "# Convert chunks into Document objects\n",
    "documents = [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
    "\n",
    "# Generate embeddings and store them in Chroma\n",
    "vectordb = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Configure Chroma as a retriever with top_k=5\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Create the retrieval chain\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Answer based on the context provided. \n",
    "context: {context}\n",
    "input: {input}\n",
    "answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# Query the chain\n",
    "response = retrieval_chain.invoke({\"input\": \"Can you recreate the tables present and show me as it is\"})\n",
    "\n",
    "# Print the answer\n",
    "print(response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
